<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Amit Kumar</title>
    <meta name="author" content="Amit Kumar">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr style="padding:0px">
          <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr style="padding:0px">
                  <td style="padding:2.5%;width:63%;vertical-align:middle">
                    <p class="name" style="text-align: center;">
                      Amit Kumar
                    </p>
                    <p>I'm a research scientist at Meta (previously Facebook), working in the Reality AI, where I my focus lie in human centric computer vision and understanding.</p>
                    <p>At Meta I've worked on Periocular authentication, 3D generative AI from self portraits, landmark tracking with transformers and realistic AI avatars.</p>
                    <p style="text-align:center">
                      <a href="mailto:iitkgp.ece.amit@hotmail.com">Email</a> &nbsp;/&nbsp;
                      <a href="data/resume_personal.pdf">CV</a> &nbsp;/&nbsp;
                      <a href="https://scholar.google.com/citations?user=qdWi6AMAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                      <a href="https://x.com/AmitKumarCV">Twitter</a> &nbsp;/&nbsp;
                      <a href="https://github.com/akumarcv">Github</a>
                    </p>
                  </td>
                  <td style="padding:2.5%;width:37%;max-width:37%">
                    <a href="images/self.JPG"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/self.JPG" class="hoverZoomLink"></a>
                  </td>
                </tr>
              </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <h2>Research</h2>
                    <p>I'm interested in computer vision, deep learning, generative AI, and image processing. Most of my research is about huamn centric computer vision, ranging from 3D generation of avatars to modeling and personalizing behaviors of these avatars. Some publications are listed below.</p>
                  </td>
                </tr>
              </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr onmouseout="power_stop()" onmouseover="power_start()">
                  <td style="padding:16px;width:20%;vertical-align:middle">
                    <div class="one">
                      <img src='images/talkingnerf.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function power_start() {
                        document.getElementById('power_image').style.opacity = "1";
                      }

                      function power_stop() {
                        document.getElementById('power_image').style.opacity = "0";
                      }
                      power_stop()
                    </script>
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle">
                    <span class="papertitle">TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans</span>
                    <br>
                    <a href="https://aggelinacha.github.io/">Aggelina Chatziagapi</a>,
                    <a href="https://bindita.github.io/">Bindita Chaudhury</a>,
                    <strong>Amit Kumar</strong>,
                    <a href="https://scholar.google.co.in/citations?hl=en&user=8KF99lYAAAAJ&view_op=list_works&sortby=pubdate">Rakesh Ranjan</a>,
                    <a href="https://www.cs.stonybrook.edu/people/faculty/dimitrissamaras">Dimitris Samaras</a>,
                    <a href="https://nsarafianos.github.io/">Nikolaos Sarafianos</a>
                    <br>
                    <em>ECCV</em>, 2024
                    <br>
                    /
                    <a href="https://arxiv.org/abs/2409.16666">arXiv</a>
                    <p></p>
                    <p>TalkinNeRF is a unified NeRF-based network that represents the holistic 4D human motion. Given a monocular video of a subject, we learn corresponding modules for the body, face, and hands, that are combined together to generate the final result.</p>
                  </td>
                </tr>

                <tr onmouseout="power_stop()" onmouseover="power_start()">
                  <td style="padding:16px;width:20%;vertical-align:middle">
                    <div class="one">
                      <img src='images/avface.jpg' width="160">
                    </div>
                    <script type="text/javascript">
                      function power_start() {
                        document.getElementById('power_image').style.opacity = "1";
                      }

                      function power_stop() {
                        document.getElementById('power_image').style.opacity = "0";
                      }
                      power_stop()
                    </script>
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle">
                    <span class="papertitle">AVFace: Towards Detailed Audio-Visual 4D Face Reconstruction</span>
                    <br>
                    <a href="https://aggelinacha.github.io/">Aggelina Chatziagapi</a>,
                    <a href="https://www.cs.stonybrook.edu/people/faculty/dimitrissamaras">Dimitris Samaras</a>,
                    <br>
                    <em>CVPR</em>, 2023
                    <br>
                    /
                    <a href="https://arxiv.org/abs/2409.16666">arXiv</a>
                    <p></p>
                    <p>AVFace incorporates both modalities (audio and video) and accurately reconstructs the 4D facial and lip motion of any speaker, without requiring any 3D ground truth for training.</p>
                  </td>
                </tr>

                <tr onmouseout="power_stop()" onmouseover="power_start()">
                  <td style="padding:16px;width:20%;vertical-align:middle">
                    <div class="one">
                      <img src='images/Hime.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function power_start() {
                        document.getElementById('power_image').style.opacity = "1";
                      }

                      function power_stop() {
                        document.getElementById('power_image').style.opacity = "0";
                      }
                      power_stop()
                    </script>
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle">
                    <span class="papertitle">HIME: Efficient Headshot Image Super-Resolution with Multiple Exemplars</span>
                    <br>
                    <a href="https://xiaoyux1ang.github.io/">Xiaoyu Xiang</a>,
                    <a href="http://www.jamorton.com/">Jon Morton</a>,
                    <a href="https://fitsumreda.github.io/">Fitsum Reda</a>,
                    <a href="https://www.linkedin.com/in/lucasdyoung/">Lucas D Young</a>,
                    <a href="https://fperazzi.github.io/">Federico Perazzi</a>,
                    <a href="https://scholar.google.co.in/citations?hl=en&user=8KF99lYAAAAJ&view_op=list_works&sortby=pubdate">Rakesh Ranjan</a>,
                    
                    <strong>Amit Kumar</strong>,
                    <a href="http://andreacolaco.info/">Andrea Colaco</a>,
                    <a href="https://scholar.google.com/citations?user=wzG_DLwAAAAJ&hl=en">Jon P Allenbach</a>
                    <br>
                    <em>WACV</em>, 2024
                    <br>
                    /
                    <a href="https://arxiv.org/abs/2409.16666">arXiv</a>
                    <p></p>
                    <p>It is challenging to make the best use of multiple exemplars: the quality and alignment of each exemplar cannot be guaranteed. Using low-quality and mismatched images as references will impair the output results. To overcome these issues, we propose the efficient Headshot Image Super-Resolution with Multiple Exemplars network (HIME) method. </p>
                  </td>
                </tr>

                <tr onmouseout="power_stop()" onmouseover="power_start()">
                  <td style="padding:16px;width:20%;vertical-align:middle">
                    <div class="one">
                      <img src='images/eyepad.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function power_start() {
                        document.getElementById('power_image').style.opacity = "1";
                      }

                      function power_stop() {
                        document.getElementById('power_image').style.opacity = "0";
                      }
                      power_stop()
                    </script>
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle">
                    <span class="papertitle">EyePAD++: A Distillation-based approach for joint Eye Authentication and Presentation Attack Detection
                      using Periocular Images</span>
                    <br>
                    <a href="https://sites.google.com/site/prithvirajdhar274/">Prithviraj Dhar</a>,
                    <strong>Amit Kumar</strong>,
                    <a href="https://www.linkedin.com/in/kirsten-kaplan-079203139/">Kirsten Kaplan</a>,
                    <a href="https://www.linkedin.com/in/khushi-gupta/">Khushi Gupta</a>,
                    <a href="https://scholar.google.co.in/citations?hl=en&user=8KF99lYAAAAJ&view_op=list_works&sortby=pubdate">Rakesh Ranjan</a>,
                    <a href="https://engineering.jhu.edu/faculty/rama-chellappa/">Rama Chellappa</a>,
                    <br>
                    <em>CVPR</em>, 2022
                    <br>
                    /
                    <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Dhar_EyePAD_A_Distillation-Based_Approach_for_Joint_Eye_Authentication_and_Presentation_CVPR_2022_paper.pdf">arxiv</a>
                    <p></p>
                    <p>we propose Eye Authentication
                      with PAD (EyePAD), a distillation-based method that trains
                      a single network for EA and PAD while reducing the effect
                      of forgetting. </p>
                  </td>
                </tr>


              </tbody>
            </table>
          </td>
        </tr>
      </tbody>
    </table>
  </body>
</html>