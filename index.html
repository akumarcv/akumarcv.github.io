<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Amit Kumar</title>
    <meta name="author" content="Amit Kumar">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr style="padding:0px">
          <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr style="padding:0px">
                  <td style="padding:2.5%;width:63%;vertical-align:middle">
                    <p class="name" style="text-align: center;">
                      Amit Kumar
                    </p>
                    <p>I'm a research scientist at Meta (previously Facebook), working in the Reality AI, where I my focus lie in human centric computer vision and understanding.</p>
                    <p>At Meta I've worked on Periocular authentication, 3D generative AI from self portraits, landmark tracking with transformers and realistic AI avatars.</p>
                    <p style="text-align:center">
                      <a href="mailto:iitkgp.ece.amit@hotmail.com">Email</a> &nbsp;/&nbsp;
                      <a href="data/resume_personal.pdf">CV</a> &nbsp;/&nbsp;
                      <a href="https://scholar.google.com/citations?user=qdWi6AMAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                      <a href="https://x.com/AmitKumarCV">Twitter</a> &nbsp;/&nbsp;
                      <a href="https://github.com/akumarcv">Github</a>
                    </p>
                  </td>
                  <td style="padding:2.5%;width:37%;max-width:37%">
                    <a href="images/self.JPG"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/self.JPG" class="hoverZoomLink"></a>
                  </td>
                </tr>
              </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:16px;width:100%;vertical-align:middle">
                    <h2>Research</h2>
                    <p>I'm interested in computer vision, deep learning, generative AI, and image processing. Most of my research is about huamn centric computer vision, ranging from 3D generation of avatars to modeling and personalizing behaviors of these avatars. Some publications are listed below.</p>
                  </td>
                </tr>
              </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr onmouseout="power_stop()" onmouseover="power_start()">
                  <td style="padding:16px;width:20%;vertical-align:middle">
                    <div class="one">
                      <img src='images/talkingnerf.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function power_start() {
                        document.getElementById('power_image').style.opacity = "1";
                      }

                      function power_stop() {
                        document.getElementById('power_image').style.opacity = "0";
                      }
                      power_stop()
                    </script>
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle">
                    <span class="papertitle">TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans</span>
                    <br>
                    <a href="https://aggelinacha.github.io/">Aggelina Chatziagapi</a>,
                    <a href="https://bindita.github.io/">Bindita Chaudhury</a>,
                    <strong>Amit Kumar</strong>,
                    <a href="https://scholar.google.co.in/citations?hl=en&user=8KF99lYAAAAJ&view_op=list_works&sortby=pubdate">Rakesh Ranjan</a>,
                    <a href="https://www.cs.stonybrook.edu/people/faculty/dimitrissamaras">Dimitris Samaras</a>,
                    <a href="https://nsarafianos.github.io/">Nikolaos Sarafianos</a>
                    <br>
                    <em>ECCV</em>, 2024
                    <br>
                    /
                    <a href="https://arxiv.org/abs/2409.16666">arXiv</a>
                    <p></p>
                    <p>TalkinNeRF is a unified NeRF-based network that represents the holistic 4D human motion. Given a monocular video of a subject, we learn corresponding modules for the body, face, and hands, that are combined together to generate the final result.</p>
                  </td>
                </tr>

                <tr onmouseout="power_stop()" onmouseover="power_start()">
                  <td style="padding:16px;width:20%;vertical-align:middle">
                    <div class="one">
                      <img src='images/avface.jpg' width="160">
                    </div>
                    <script type="text/javascript">
                      function power_start() {
                        document.getElementById('power_image').style.opacity = "1";
                      }

                      function power_stop() {
                        document.getElementById('power_image').style.opacity = "0";
                      }
                      power_stop()
                    </script>
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle">
                    <span class="papertitle">AVFace: Towards Detailed Audio-Visual 4D Face Reconstruction</span>
                    <br>
                    <a href="https://aggelinacha.github.io/">Aggelina Chatziagapi</a>,
                    <a href="https://www.cs.stonybrook.edu/people/faculty/dimitrissamaras">Dimitris Samaras</a>,
                    <br>
                    <em>CVPR</em>, 2023
                    <br>
                    /
                    <a href="https://arxiv.org/abs/2409.16666">arXiv</a>
                    <p></p>
                    <p>AVFace incorporates both modalities (audio and video) and accurately reconstructs the 4D facial and lip motion of any speaker, without requiring any 3D ground truth for training.</p>
                  </td>
                </tr>

                <tr onmouseout="power_stop()" onmouseover="power_start()">
                  <td style="padding:16px;width:20%;vertical-align:middle">
                    <div class="one">
                      <img src='images/Hime.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function power_start() {
                        document.getElementById('power_image').style.opacity = "1";
                      }

                      function power_stop() {
                        document.getElementById('power_image').style.opacity = "0";
                      }
                      power_stop()
                    </script>
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle">
                    <span class="papertitle">HIME: Efficient Headshot Image Super-Resolution with Multiple Exemplars</span>
                    <br>
                    <a href="https://xiaoyux1ang.github.io/">Xiaoyu Xiang</a>,
                    <a href="http://www.jamorton.com/">Jon Morton</a>,
                    <a href="https://fitsumreda.github.io/">Fitsum Reda</a>,
                    <a href="https://www.linkedin.com/in/lucasdyoung/">Lucas D Young</a>,
                    <a href="https://fperazzi.github.io/">Federico Perazzi</a>,
                    <a href="https://scholar.google.co.in/citations?hl=en&user=8KF99lYAAAAJ&view_op=list_works&sortby=pubdate">Rakesh Ranjan</a>,
                    
                    <strong>Amit Kumar</strong>,
                    <a href="http://andreacolaco.info/">Andrea Colaco</a>,
                    <a href="https://scholar.google.com/citations?user=wzG_DLwAAAAJ&hl=en">Jon P Allenbach</a>
                    <br>
                    <em>WACV</em>, 2024
                    <br>
                    /
                    <a href="https://arxiv.org/abs/2409.16666">arXiv</a>
                    <p></p>
                    <p>It is challenging to make the best use of multiple exemplars: the quality and alignment of each exemplar cannot be guaranteed. Using low-quality and mismatched images as references will impair the output results. To overcome these issues, we propose the efficient Headshot Image Super-Resolution with Multiple Exemplars network (HIME) method. </p>
                  </td>
                </tr>

                <tr onmouseout="power_stop()" onmouseover="power_start()">
                  <td style="padding:16px;width:20%;vertical-align:middle">
                    <div class="one">
                      <img src='images/eyepad.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function power_start() {
                        document.getElementById('power_image').style.opacity = "1";
                      }

                      function power_stop() {
                        document.getElementById('power_image').style.opacity = "0";
                      }
                      power_stop()
                    </script>
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle">
                    <span class="papertitle">EyePAD++: A Distillation-based approach for joint Eye Authentication and Presentation Attack Detection
                      using Periocular Images</span>
                    <br>
                    <a href="https://sites.google.com/site/prithvirajdhar274/">Prithviraj Dhar</a>,
                    <strong>Amit Kumar</strong>,
                    <a href="https://www.linkedin.com/in/kirsten-kaplan-079203139/">Kirsten Kaplan</a>,
                    <a href="https://www.linkedin.com/in/khushi-gupta/">Khushi Gupta</a>,
                    <a href="https://scholar.google.co.in/citations?hl=en&user=8KF99lYAAAAJ&view_op=list_works&sortby=pubdate">Rakesh Ranjan</a>,
                    <a href="https://engineering.jhu.edu/faculty/rama-chellappa/">Rama Chellappa</a>,
                    <br>
                    <em>CVPR</em>, 2022
                    <br>
                    /
                    <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Dhar_EyePAD_A_Distillation-Based_Approach_for_Joint_Eye_Authentication_and_Presentation_CVPR_2022_paper.pdf">arxiv</a>
                    <p></p>
                    <p>We propose Eye Authentication
                      with PAD (EyePAD), a distillation-based method that trains
                      a single network for EA and PAD while reducing the effect
                      of forgetting. </p>
                  </td>
                </tr>
                
                <tr onmouseout="power_stop()" onmouseover="power_start()">
                  <td style="padding:16px;width:20%;vertical-align:middle">
                    <div class="one">
                      <img src='images/evrnet.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function power_start() {
                        document.getElementById('power_image').style.opacity = "1";
                      }

                      function power_stop() {
                        document.getElementById('power_image').style.opacity = "0";
                      }
                      power_stop()
                    </script>
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle">
                    <span class="papertitle">EVRNet: Efficient Video Restoration on Edge Devices</span>
                    <br>
                    <a href="https://sacmehta.github.io/">Sachin Mehta</a>,
                    <strong>Amit Kumar</strong>,
                    <a href="https://fitsumreda.github.io/">Fitsum Reda</a>,
                    <a href="https://www.linkedin.com/in/varun-nasery-5353341a/">Varun Nasery</a>,
                    <a href="https://scholar.google.com/citations?user=jKO4YPsAAAAJ&hl=en">Vikram Mulukutla</a>,
                    <a href="https://scholar.google.co.in/citations?hl=en&user=8KF99lYAAAAJ&view_op=list_works&sortby=pubdate">Rakesh Ranjan</a>,
                    <a href="https://v-chandra.github.io/">Vikas Chandra</a>,
                    
                    <br>
                    <em>ACM-MM</em>, 2021
                    <br>
                    /
                    <a href="https://arxiv.org/pdf/2012.02228">arxiv</a>
                    <p></p>
                    <p>To restore
                      videos on recipient edge devices in real-time, we introduce an efficient video restoration network, EVRNet. EVRNet efficiently allocates parameters inside the network using alignment, differential, and fusion modules. </p>
                  </td>
                </tr>

                <tr onmouseout="power_stop()" onmouseover="power_start()">
                  <td style="padding:16px;width:20%;vertical-align:middle">
                    <div class="one">
                      <img src='images/RAE.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function power_start() {
                        document.getElementById('power_image').style.opacity = "1";
                      }

                      function power_stop() {
                        document.getElementById('power_image').style.opacity = "0";
                      }
                      power_stop()
                    </script>
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle">
                    <span class="papertitle">Semi-Supervised Landmark-Guided Restoration of Atmospheric Turbulent Images</span>
                    <br>
                    <a href="https://sunanditapatra.wixsite.com/camp">Sunandita Patra</a>,
                    <a href="https://www.linkedin.com/in/james-cary-mason/">James Mason</a>,
                    <strong>Amit Kumar</strong>,
                    <a href="https://homepages.laas.fr/malik/Home/Home.html">Malik Ghallab</a>,
                    <a href="https://www.fbk.eu/en/paolo-traverso/">Paolo Traverso</a>,
                    <a href="https://www.cs.umd.edu/~nau/">Dana Nau</a>,
                    
                    <br>
                    <em>ICAPS</em>, 2020
                    <br>
                    /
                    <a href="https://ojs.aaai.org/index.php/ICAPS/article/view/6743/6597">arxiv</a>
                    <p></p>
                    <p>RAE uses hierarchical operational models to perform tasks in dynamically changing environments. </p>
                  </td>
                </tr>
                
                <tr onmouseout="power_stop()" onmouseover="power_start()">
                  <td style="padding:16px;width:20%;vertical-align:middle">
                    <div class="one">
                      <img src='images/at.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function power_start() {
                        document.getElementById('power_image').style.opacity = "1";
                      }

                      function power_stop() {
                        document.getElementById('power_image').style.opacity = "0";
                      }
                      power_stop()
                    </script>
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle">
                    <span class="papertitle">Semi-Supervised Landmark-Guided Restoration of
                      Atmospheric Turbulent Images</span>
                    <br>
                    <a href="https://samuel930930.github.io/">Samuel Chun Pong Lau</a>,
                    <strong>Amit Kumar</strong>,
                    <a href="https://engineering.jhu.edu/faculty/rama-chellappa/">Rama Chellappa</a>,
                    
                    <br>
                    <em>IEEE Journal of Selected Topics in Signal Processing (JSTSP), 2021</em>, 2020
                    <br>
                    /
                    <a href="https://ieeexplore.ieee.org/abstract/document/9320575">IEEE</a>
                    <p></p>
                    <p>A semisupervised method for jointly extracting facial landmarks and restoring the degraded images by exploiting the semantic information from the landmarks. </p>
                  </td>
                </tr>

                <tr onmouseout="power_stop()" onmouseover="power_start()">
                  <td style="padding:16px;width:20%;vertical-align:middle">
                    <div class="one">
                      <img src='images/s2ld.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function power_start() {
                        document.getElementById('power_image').style.opacity = "1";
                      }

                      function power_stop() {
                        document.getElementById('power_image').style.opacity = "0";
                      }
                      power_stop()
                    </script>
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle">
                    <span class="papertitle">S2LD: Semi-Supervised Landmark Detection in Low Resolution Images
                      and Impact on Face Verification</span>
                    <br>
                    <strong>Amit Kumar</strong>,
                    <a href="https://engineering.jhu.edu/faculty/rama-chellappa/">Rama Chellappa</a>,
                    <br>
                    <em>CVPR-W</em>, 2020
                    <br>
                    /
                    <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w45/Kumar_S2LD_Semi-Supervised_Landmark_Detection_in_Low-Resolution_Images_and_Impact_on_CVPRW_2020_paper.pdf">arxiv</a>
                    <p></p>
                    <p>Predicting landmarks
                      directly on low resolution images is more effective than the
                      current practice of aligning images after rescaling or super-resolution. </p>
                  </td>
                </tr>
                
                <tr onmouseout="power_stop()" onmouseover="power_start()">
                  <td style="padding:16px;width:20%;vertical-align:middle">
                    <div class="one">
                      <img src='images/cvprw19.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function power_start() {
                        document.getElementById('power_image').style.opacity = "1";
                      }

                      function power_stop() {
                        document.getElementById('power_image').style.opacity = "0";
                      }
                      power_stop()
                    </script>
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle">
                    <span class="papertitle">Attention Driven Vehicle Re-identification and Unsupervised Anomaly Detection
                      for Traffic Understanding</span>
                    <br>
                    <a href="https://pirazh.github.io/">Pirazh Khorramshahi</a>,
                    <a href="https://www.neeharperi.com/">Neehar Peri</a>,
                    <strong>Amit Kumar</strong>,
                    <a href="https://anshulbshah.github.io/">Anshul Shah</a>,
                    <a href="https://engineering.jhu.edu/faculty/rama-chellappa/">Rama Chellappa</a>,
                    <br>
                    <em>CVPR Nvidia City Challenge<strong> (Oral)</strong></em>, 2019
                    <br>
                    /
                    <a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/AI%20City/Khorramshahi_Attention_Driven_Vehicle_Re-identification_and_Unsupervised_Anomaly_Detection_for_Traffic_CVPRW_2019_paper.pdf">arxiv</a>
                    <p></p>
                    <p>We leverage an attention-based model
                      which learns to focus on different parts of a vehicle by conditioning the feature maps on visible key-points. We use
                      triplet embedding to reduce the dimensionality of the features obtained from the ensemble of networks trained using different datasets.</p>
                  </td>
                </tr>

                <tr onmouseout="power_stop()" onmouseover="power_start()">
                  <td style="padding:16px;width:20%;vertical-align:middle">
                    <div class="one">
                      <img src='images/AAVER.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function power_start() {
                        document.getElementById('power_image').style.opacity = "1";
                      }

                      function power_stop() {
                        document.getElementById('power_image').style.opacity = "0";
                      }
                      power_stop()
                    </script>
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle">
                    <span class="papertitle">A Dual-Path Model With Adaptive Attention For Vehicle Re-Identification</span>
                    <br>
                    <a href="https://pirazh.github.io/">Pirazh Khorramshahi</a>,
                    <strong>Amit Kumar</strong>,
                    <a href="https://www.neeharperi.com/">Neehar Peri</a>,
                    <a href="https://rssaketh.github.io/">Sai Saketh Rambhatla</a>,
                    <a href="https://homepage.citi.sinica.edu.tw/pages/pullpull/index_en.html">Jun-Cheng Chen</a>,
                    <a href="https://engineering.jhu.edu/faculty/rama-chellappa/">Rama Chellappa</a>,
                    <br>
                    <em>ICCV <strong> (Oral)</strong></em>, 2020
                    <br>
                    /
                    <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Khorramshahi_A_Dual-Path_Model_With_Adaptive_Attention_for_Vehicle_Re-Identification_ICCV_2019_paper.pdf">arxiv</a>
                    <p></p>
                    <p>In AAVER, the global appearance path captures macroscopic vehicle features while the
                      orientation conditioned part appearance path learns to capture localized discriminative features by focusing attention
                      on the most informative key-points.</p>
                  </td>
                </tr>

                <tr onmouseout="power_stop()" onmouseover="power_start()">
                  <td style="padding:16px;width:20%;vertical-align:middle">
                    <div class="one">
                      <img src='images/PCDCNN.png' width="160">
                    </div>
                    <script type="text/javascript">
                      function power_start() {
                        document.getElementById('power_image').style.opacity = "1";
                      }

                      function power_stop() {
                        document.getElementById('power_image').style.opacity = "0";
                      }
                      power_stop()
                    </script>
                  </td>
                  <td style="padding:8px;width:80%;vertical-align:middle">
                    <span class="papertitle">Disentangling 3D Pose in A Dendritic CNN
                      for Unconstrained 2D Face Alignment</span>
                    <br>
                    <strong>Amit Kumar</strong>,
                    <a href="https://engineering.jhu.edu/faculty/rama-chellappa/">Rama Chellappa</a>,
                    <br>
                    <em>CVPR</em>, 2018
                    <br>
                    /
                    <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Kumar_Disentangling_3D_Pose_CVPR_2018_paper.pdf">CVPR</a>
                    <p></p>
                    <p>Following a Bayesian formulation, we disentangle the 3D pose of a face image explicitly by conditioning the landmark estimation on pose, making it different from multi-tasking approaches.</p>
                  </td>
                </tr>

              </tbody>
            </table>
          </td>
        </tr>
      </tbody>
    </table>
  </body>
</html>